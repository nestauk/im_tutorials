{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community Detection in Topic Cooccurrence Networks: Finding Research Disciplines in UK Research Projects\n",
    "\n",
    "This tutorial looks at the use of [cooccurrence networks](https://en.wikipedia.org/wiki/Co-occurrence_network) and community detection to identify academic disciplines from research projects in the [Gateway to Research](https://gtr.ukri.org/) database. \n",
    "\n",
    "Disciplines are high level subject areas, such as _biological science_ or _engineering_ (they might map well to the faculties of a university). The Gateway to Research data does not categorise projects at this level, however in many cases it is useful to do so. We may want to break down research funding according to discipline, see which projects are multi-disciplinary, or understand the differences in the nature of research outputs in different fields.\n",
    "\n",
    "**Tutorial Structure**\n",
    "\n",
    "- [Preamble](#Preamble)\n",
    "- [Import Data](#Preamble)\n",
    "- [Discipline Identification Through Community Detection](#Discipline-Identification-Through-Community-Detection)\n",
    " - [Cooccurrence Networks](#Cooccurrence-Networks)\n",
    " - [Normalising Edge Weights](#Normalising-Edge-Weights)\n",
    " - [Building a Network](#Building-a-Network)\n",
    " - [Community Detection](#Community-Detection)\n",
    " - [Interactive Network Visualisation](#Interactive-Network-Visualisation)\n",
    " - [Investigating the Communities](#Investigating-the-Communities)\n",
    " - [Assigning Disciplines to Projects](#Assigning-Disciplines-to-Projects)\n",
    "- [Analysis](#Analysis)\n",
    " - [Interdisciplinarity](#Interdisciplinarity)\n",
    " - [Discipline and Funding Bodies](#Discipline-and-Funding-Bodies)\n",
    "- [Conclusions](#Conclusions)\n",
    "- [Extra: Aggregate Community Detection](#Extra:-Aggregate-Community-Detection)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# install im_tutorial package\n",
    "!pip install git+https://github.com/nestauk/im_tutorials.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing useful Python utility libraries we'll need\n",
    "import ast\n",
    "import smart_open\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "\n",
    "# matplotlib for static plots\n",
    "import matplotlib.pyplot as plt\n",
    "# numpy for mathematical functions\n",
    "import numpy as np\n",
    "# pandas for handling tabular data\n",
    "import pandas as pd\n",
    "\n",
    "from im_tutorials.utilities import chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this project is stored as a csv on Amazon Web Services (AWS) S3, a static cloud file storage service. We can use `pandas` to pull the data directly into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='innovation-mapping-tutorials'\n",
    "gtr_projects_key='gateway-to-research/gtr_projects.csv'\n",
    "list_cols = ['research_topics', 'research_subjects']\n",
    "# We use ast.literal_eval to convert the two columns above from\n",
    "# string representations of lists to actual lists.\n",
    "gtr_projects_df = pd.read_csv(\n",
    "    smart_open.smart_open(f'https://s3.us-east-2.amazonaws.com/{bucket}/{gtr_projects_key}'),\n",
    "    converters={k: ast.literal_eval for k in list_cols}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look at the top few rows of data shows us the fields and the format of the data within each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a first glance, The data in the research topic and research subject fields look fairly similar. For every project, both fields contain a lists of terms, which appear similar in content. From a first glance, it looks like the topics may be more granular than the subjects. We can count how many unique terms there are in each field to find out if that might be true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the lists of research subjects and elements and count the contents\n",
    "research_subject_counter = Counter(itertools.chain(*gtr_projects_df['research_subjects']))\n",
    "research_topic_counter = Counter(itertools.chain(*gtr_projects_df['research_topics']))\n",
    "print('There are {} unique research subjects in the GtR projects dataset.'.format(len(research_subject_counter)))\n",
    "print('There are {} unique research topics in the GtR projects dataset.'.format(len(research_topic_counter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we were probably right. There are 82 research subjects, while there are over 600 research topics, indicating that these might be a finer representation of the contents of each project.\n",
    "\n",
    "Let's also have a look at the frequencies of each subject and topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top Research Subjects by Frequency\", '\\n')\n",
    "print('{:<40}Frequency'.format(\"Topic\"))\n",
    "for k, v in research_subject_counter.most_common(10):\n",
    "    print(f'{k:<40}{v}')\n",
    "    \n",
    "print('\\nMedian Topic Freqency:')\n",
    "print(np.median(list(research_subject_counter.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top Research Topics by Frequency\", '\\n')\n",
    "print('{:<40}Frequency'.format('Topic'))\n",
    "for k, v in research_topic_counter.most_common(10):\n",
    "    print(f'{k:<40}{v}')\n",
    "    \n",
    "print('\\nMedian Topic Freqency:')\n",
    "print(np.median(list(research_topic_counter.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the top research subject is _Info. & commun. Technol._ and the top research topic is _Climate & Climate Change_, both by some margin. However, we can also see that the top spots are populated by subjects and topics from several disciplines. 50% of the subjects occur 575 times or fewer, while for topics the median frequency is 69.\n",
    "\n",
    "While the research topics and subjects are useful as keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discipline Identification Through Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cooccurrence Networks\n",
    "\n",
    "We are going to define communities of research topics as groups of topics which commonly occur together. An effective way of finding these clusters, and visualising the results, is by creating a topic cooccurrence network.\n",
    "\n",
    "A cooccurrence graph is a network structure, where nodes are elements and an edge represents the elements of two nodes having cooccured at least once. The edges can then be \"weighted\" by the frequencies of each cooccurring pair. In the case of our research projects, we can say that two topics have cooccurred if they appear in at least one project together. To find all cooccurrences we therefore need to find the pairwise combinations of research topics for every project. For example, a single project with the topics\n",
    "```\n",
    "['Materials Characterisation', 'High Performance Computing', 'Condensed Matter Physics']\n",
    "```\n",
    "\n",
    "will become a set of topic pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combinations function from itertools generates all the possible\n",
    "# elements of combinations from a list with length  r.\n",
    "list(itertools.combinations(['Materials Characterisation', 'High Performance Computing', 'Condensed Matter Physics'], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These cooccurrences would form a triangular network; 3 nodes and 3 edges, where each edge has a frequency weight of 1.\n",
    "\n",
    "Let's now imagine that we have several projects, and we repeat this process for each of them in turn. We will generate a list of cooccurring pairs, which we can then turn into a small cooccurrence network. The image below shows the cooccurrence network that is generated by applying this method to 3 projects. We can see that:\n",
    "\n",
    "- Project 1 forms a single cooccurring pair.\n",
    "- _Economic & Social History_ and _Music & Society_ are present in more than one project and bridge groups of topics that have not appeared together.\n",
    "\n",
    "<img src=\"https://github.com/nestauk/im_tutorials/blob/master/img/topic_cooccurrence_network.png?raw=true\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "It is easy to see how repeating this process across hundreds or thousands of projects could quickly build up a picture of which topics commonly cooccur and form clusters that we might be able to identify as subjects or disciplines.\n",
    "\n",
    "To create a cooccurrence network across all projects in our dataset, we will use a Python list comprehension, and then chain togeher all of the cooccurring pairs into one long list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate every pair combination of research topics from each project.\n",
    "# Each pair is sorted alphabetically to make sure that there is only one \n",
    "# possible permutation of each edge.\n",
    "cooccurrences = []\n",
    "\n",
    "for topics in gtr_projects_df['research_topics']:\n",
    "    topic_pairs = itertools.combinations(topics, 2)\n",
    "    for pair in topic_pairs:\n",
    "        cooccurrences.append(tuple(sorted(pair)))\n",
    "\n",
    "# The same can be achieved in this one-liner\n",
    "# cooccurrences = list(\n",
    "# chain(*[[tuple(sorted(c)) for c in (itertools.combinations(d, 2))] for d in gtr_projects_df['research_topics']])\n",
    "# )\n",
    "\n",
    "# Count the frequency of each cooccurring pair.\n",
    "research_topic_co_counter = Counter(cooccurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top Research Topic Cooccurrences by Frequency\", '\\n')\n",
    "print('{:<70}{}'.format('Cooccurrence', 'Frequency'))\n",
    "for k, v in research_topic_co_counter.most_common(20):\n",
    "    topics = k[0] + ' + ' + k[1]\n",
    "    print(f'{topics:<70}{v}')\n",
    "    \n",
    "print('\\nMedian Topic Cooccurrence Freqency:')\n",
    "print(np.median(list(research_topic_co_counter.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising Edge Weights\n",
    "\n",
    "Looking at the most frequently cooccurring topics we can pairs that make intuitive sense and are all generally captured neatly within higher order academic disciplines.\n",
    "\n",
    "However this, along with the individual topic frequencies, also shows us that using the cooccurrence frequency as our edge weight might not be such a good idea. High frequency elements are simply more likely to cooccur due to chance. Therefore we should normalise our edge weights. One method for this is to calculate the association strength is a an edge weight where the cooccurrence freqency is normalised by the product of the individual terms' occurrence counts. It is defined as\n",
    "\n",
    "$$ a = \\frac{2 n c_{ij}}{o_{i}o_{j}} $$\n",
    "\n",
    "where $n$ is the total number of elements, $c_{ij}$ is the number of cooccurrences between elements $i$ and $j$, and $o_{i}$ and $o_{j}$ are the individual frequency counts of each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_strength(combo, occurrences, cooccurrences, total):\n",
    "    '''association_strength\n",
    "    Calculates the association strength between a cooccurring pair.\n",
    "    '''\n",
    "    a_s = ((2 * total * cooccurrences[combo]) / \n",
    "           (occurrences[combo[0]] * occurrences[combo[1]]))\n",
    "    return a_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build our cooccurrence network, we need to generate a list of unique edges from our long list of cooccurrences and then calculate the association strength for each edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set of cooccurences (a list of unique pairs).\n",
    "# This will form the edges of our cooccurrence graph.\n",
    "edges = set(cooccurrences)\n",
    "# Calculate the total number of elements\n",
    "n = len(list(itertools.chain(*gtr_projects_df['research_topics'])))\n",
    "# Calculate the association strength for each edge.\n",
    "# We take the log of the association strength to give it\n",
    "# a normal distribution.\n",
    "assoc_strengths = np.log10([association_strength(\n",
    "    edge,\n",
    "    research_topic_counter, \n",
    "    research_topic_co_counter, \n",
    "    n) for edge in edges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(assoc_strengths, bins=100)\n",
    "ax.set_xlabel('Association Strength')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the association strengths shows a fairly smooth normal distribution. We can see that without applying a logarithm, there would be weights in our graph 100,000 times larger than others!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has 3 main tools for working with networks: [`networkx`](https://networkx.github.io/), [`igraph`](https://igraph.org/redirect.html) and [`graph-tool`](https://graph-tool.skewed.de). The first of these, `networkx`, is easy to install and interacting with it is straightforward. It is suitable for networks with up to hundreds of thousands of nodes or edges. With very large networks, it is recommended to use `graph-tool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add the edges, we simply create a list of tuples that represent our edges, with each containing the source node `s`, the target node `t`, and the association strength `a_s`. We then instantiate a `networkx` `Graph` object, and simply use the method `.add_weighted_edges_from()` to put the list of edges into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_edges = []\n",
    "for (s, t), a_s in zip(edges, assoc_strengths):\n",
    "    weighted_edges.append((s, t, a_s))\n",
    "\n",
    "g = nx.Graph()\n",
    "g.add_weighted_edges_from(weighted_edges, weight='association_strength')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then call on an edge in the graph to view its properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.edges[('Materials Characterisation', 'Materials Synthesis & Growth')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community detection is the process of finding sets of nodes in a network that are densely internally. Algorithms for this process generally find the boundaries of communities by analysing the density of connections between a group of nodes with respect to the density of connections outside of this group. A pair of nodes is more likely to be connected if they are both members of the same community.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/nestauk/im_tutorials/blob/master/img/community_detection.png?raw=true\" alt=\"communities\" width=200>\n",
    "\n",
    "There are [many different types of community detection](https://github.com/benedekrozemberczki/awesome-community-detection). Here we will use the Louvain Method, as there is an actively maintained, easy to use Pyton implementation, [`python-louvain`](https://python-louvain.readthedocs.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `python-louvain` imports as `community`\n",
    "import community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find which community each research topic is in, we apply `best_partition` to our cooccurrence network. We can vary the resolution to change granular the community detection is. We also pass in the name of the edge weight that we want the method to use when determining where community boundaries are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = community.best_partition(g, resolution=0.6, random_state=42, weight='association_strength')\n",
    "n_communities = len(set(part.values()))\n",
    "print('{} communities detected.'.format(n_communities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Network Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a cooccurrence network with each node assigned to a community. This seems like a nice place to visualise our output so far. To do this, we will use [`bokeh`](https://bokeh.pydata.org/en/latest/), a Python library that allows the user to create interactive plots, and is based on the popular plotting library `D3`, which powers many visualisations on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.palettes import Category20, Spectral4\n",
    "from bokeh.models import Circle, MultiLine, HoverTool, TapTool\n",
    "from bokeh.models.graphs import from_networkx, NodesAndLinkedEdges\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we make the plot, we will add some extra properties to the nodes in our network. First, we will give each node an attribute, `topic_name`, which is the name of the research topic that the node represents. Second, we will give the node a colour based on the community to which it belongs.\n",
    "\n",
    "Note: This code will break if more than 20 communities are used. In this situation a different colour palette would be needed, or a different way of selecting colours from a small palette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {k: k for k, _ in part.items()}\n",
    "nx.set_node_attributes(g, names, name='topic_name')\n",
    "community_colors = {k: Category20[n_communities][c] for k, c in part.items()}\n",
    "nx.set_node_attributes(g, community_colors, name='color')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print a node to see the properties it holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.nodes['Materials Characterisation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot our network on a 2 dimensional plane, we will need to calculate coordinates for each node. There are read-made algorithms for positioning network nodes visually, and some are built in to `networkx`. The spring layout tries to position nodes according to their edges and relative levels of attraction based on edge weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(g, weight='association_strength', scale=2, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need to make a nice plot. Luckily, `bokeh` has built-in support for `networkx` graphs, which makes plotting and interacting with them easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot and give it some basic features.\n",
    "plot = figure(title=\"Research Topic Cooccurrence Network\",\n",
    "              x_range=(-2.1,2.1), y_range=(-2.1,2.1),\n",
    "             )\n",
    "\n",
    "# Use the renderer built in to `bokeh` to transform our Graph\n",
    "# object into something that `bokeh` can plot.\n",
    "graph_renderer = from_networkx(g, pos, center=(0,0))\n",
    "# Draw glyphs for our nodes and assign properties for interactions.\n",
    "graph_renderer.node_renderer.glyph = Circle(size=7, fill_color='color', line_color=None)\n",
    "graph_renderer.node_renderer.selection_glyph = Circle(size=7, fill_color='color')\n",
    "graph_renderer.node_renderer.hover_glyph = Circle(size=7, fill_color='color')\n",
    "graph_renderer.node_renderer.muted_glyph = Circle(size=7, fill_color='color', fill_alpha=0.9)\n",
    "# Draw glyphs for edges and assign properties for interactions.\n",
    "graph_renderer.edge_renderer.glyph = MultiLine(line_color=\"#CCCCCC\", line_alpha=0.2, line_width=1)\n",
    "graph_renderer.edge_renderer.selection_glyph = MultiLine(line_color=Spectral4[2], line_width=1.5)\n",
    "graph_renderer.edge_renderer.hover_glyph = MultiLine(line_color=Spectral4[1], line_width=1.5)\n",
    "# Add the ability to select nodes.\n",
    "graph_renderer.selection_policy = NodesAndLinkedEdges()\n",
    "# Add a hover tool, that allows us to investigate nodes with a tooltip. \n",
    "node_hover_tool = HoverTool(tooltips=[(\"Topic\", \"@topic_name\")])\n",
    "# Put everything on the plot.\n",
    "plot.add_tools(node_hover_tool, TapTool())\n",
    "plot.renderers.append(graph_renderer)\n",
    "\n",
    "show(plot)\n",
    "\n",
    "# Uncomment this line if using google colab\n",
    "# output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart shows us every research topic and the cooccurrence edges between them. The colours correspond to the community membership of a node. Hovering over a node allows us to see its name, while clicking on it shows us the edges that connect it to other nodes. `bokeh` also allows you to pan and zoom the plot and save the output.\n",
    "\n",
    "**Pause**\n",
    "- How might the techniques we have used to far apply to a question or dataset that you have?\n",
    "- What other entities might you be able to put in to a cooccurrence network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the Communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's manually inspect the topics in each community to see if we can see what disciplines they might form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_part = defaultdict(list)\n",
    "for k, v in part.items():\n",
    "    reverse_part[v].append(k)\n",
    "    \n",
    "for c, topics in reverse_part.items():\n",
    "    print(c)\n",
    "    for chunk in chunks(topics, 4):\n",
    "        print(', '.join(chunk))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a community ID to discipline mapping.\n",
    "\n",
    "Fill this in with the disciplines that you have identified in the printed topics list above. You may find that two or more communities form part of the same discipline. The community detection algorithm may have identified a slightly different number of communities depending on your the environment that your notebook is running in - adjust the number of key-value pairs in the `community_discipline_map` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_discipline_map = {\n",
    "    0: '',\n",
    "    1: '',\n",
    "    2: '',\n",
    "    3: '',\n",
    "    4: '',\n",
    "    5: '',\n",
    "    6: '',\n",
    "    7: '',\n",
    "    8: '',\n",
    "    9: '',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Disciplines to Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to do any analysis on the research projects using the discipline as a feature, we need to label each project with the correct discipline, according the its research topics.\n",
    "\n",
    "The first step is to map each topic to the discipline community that it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_discipline_mapping = {top: community_discipline_map[disc] for top, disc in part.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have this mapping we can:\n",
    "\n",
    "1. Apply it to the research topics\n",
    "    - `['Sociology', 'Economics', 'Information & Knowledge Mgmt']` might become `['social', 'social', 'maths_computing_ee']`\n",
    "2. Get the unique set of disciplines for each project\n",
    "    - `['social', 'social', 'maths_computing_ee']` becomes `{'social', 'maths_computing_ee'}`\n",
    "3. Count the number of disciplines in each project\n",
    "4. Flag projects that are mono-disciplinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map topics to disciplines using pandas' apply method on\n",
    "# the `research_topics` column.\n",
    "gtr_projects_df['disciplines'] = gtr_projects_df['research_topics'].apply(\n",
    "    lambda x: [topic_discipline_mapping[val] for val in x])\n",
    "gtr_projects_df['discipline_set'] = [set(d) for d in gtr_projects_df['disciplines']]\n",
    "\n",
    "# Projects funded by MRC and NC3Rs have no research topics\n",
    "# We will make the assumption that they are all medical_sciences\n",
    "gtr_projects_df['discipline_set'][\n",
    "    (gtr_projects_df['funder_name'] == 'MRC') | \n",
    "    (gtr_projects_df['funder_name'] == 'NC3Rs')] = set(['medical_sciences'])\n",
    "# Count the number of unique disciplines for each project\n",
    "gtr_projects_df['n_disciplines'] = [len(x) for x in gtr_projects_df['discipline_set']]\n",
    "# Create a field that flags whether a discipline is mono-disciplinary\n",
    "gtr_projects_df['is_single_discipline'] = [True if len(x)==1 else np.nan if len(x)==0 else False \n",
    "                                           for x in gtr_projects_df['discipline_set']]\n",
    "\n",
    "print('{:.2f}% of projects are mono-disciplinary.'.format(gtr_projects_df['is_single_discipline'].mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discipline_counter = Counter(itertools.chain(*gtr_projects_df['discipline_set']))\n",
    "\n",
    "print(\"Disciplines by Frequency\", '\\n')\n",
    "print('{:<40}{}'.format('Discipline', 'Frequency'))\n",
    "for discipline, count in discipline_counter.most_common(20):\n",
    "    print(f'{discipline:<40}{count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have another look at our dataframe now that we've added these extra research discipline fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our projects labelled by discipline, we can do some analysis.\n",
    "\n",
    "### Interdisciplinarity\n",
    "\n",
    "First, we are going to look at which disciplines are commonly found together in research projects to see what the landscape of interdisciplinary research is like in the UK.\n",
    "\n",
    "To do this, we are going to apply our method for finding cooccurring pairs of entities to the `discipline_set` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time we apply our one-liner to find cooccurring disciplines\n",
    "discipline_cooccurrences = list(\n",
    "    itertools.chain(*[[tuple(sorted(c)) for c in itertools.combinations(d, 2)] for d in gtr_projects_df['discipline_set']])\n",
    ")\n",
    "# Count the frequency of each cooccurring pair.\n",
    "discipline_edge_counter = Counter(discipline_cooccurrences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a pivot table of our discipline pair counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discipline_cooccurrence_df = pd.DataFrame({\n",
    "    'subj0': [dcc[0] for dcc in discipline_edge_counter.keys()],\n",
    "    'subj1': [dcc[1] for dcc in discipline_edge_counter.keys()],\n",
    "    'count': list(discipline_edge_counter.values()),\n",
    "}).pivot_table(index='subj0', columns='subj1')['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn is a plotting library based on matplotlib\n",
    "# It has lots of nice presets for statistical plotting\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot a heatmap of the frequency of disciplinary pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_discipline_labels(labels):\n",
    "    return [l.get_text().replace('_', ' ').title() for l in labels]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "sns.heatmap(discipline_cooccurrence_df, annot=True, fmt='.0f', ax=ax, cbar=None, cmap='viridis')\n",
    "ax.set_xticklabels(format_discipline_labels(ax.get_xticklabels()), rotation=30, ha='right')\n",
    "ax.set_yticklabels(format_discipline_labels(ax.get_yticklabels()))\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(None)\n",
    "ax.set_title('Discipline Crossover in Multidiscplinary Projects')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're at it, let's look at the distribution of disciplinarity among the projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "gtr_projects_df['n_disciplines'].value_counts().plot.bar(color='C0', ax=ax)\n",
    "ax.set_xlabel('N Disciplines')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Project Frequency by Discipline Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disciplines and Funding Bodies\n",
    "\n",
    "It could be argued that we could infer the discipline of a project from the funding body. Besides the fact that we may want a slightly higher level of domain granularity than that offered by the funding bodies, it also excludes the overlap of disciplines between funders that exists in the real world.\n",
    "\n",
    "Let's have a look at how our disciplines match up against the funders. To do this, we will create another heatmap, this time plotting the fraction of projects that contain a discipline, broken down by funding body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "discipline_binarized = mlb.fit_transform(gtr_projects_df['discipline_set'])\n",
    "discipline_binarized_df = pd.DataFrame(discipline_binarized, columns=mlb.classes_)\n",
    "\n",
    "# Group projects by funder and calculate the frequencies of disciplines\n",
    "# then normalise by the total number of projects for each funder (rows add to 100)\n",
    "funder_discipline_df = discipline_binarized_df.groupby(gtr_projects_df['funder_name']).sum().divide(\n",
    "    discipline_binarized_df.groupby(gtr_projects_df['funder_name']).sum().sum(axis=1), axis=0) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4.5))\n",
    "sns.heatmap(funder_discipline_df, annot=True, fmt='.0f', ax=ax, cmap='viridis')\n",
    "ax.set_title('Percentages of Projects Containing a Discipline by Funder')\n",
    "ax.set_xlabel('Discipline')\n",
    "ax.set_ylabel('Funder')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AHRC (Arts and Humanities Research Council)\n",
    "- BBSRC (Biotechnology and Biological Sciences Research Council)     \n",
    "- EPSRC (Engineering and Physical Sciences Research Council) \n",
    "- ESRC (Economic and Social Research Council) \n",
    "- JISC (Joint Information Systems Committee) \n",
    "- MRC (Medical Research Council)\n",
    "- NC3Rs (The National Centre for the 3Rs)\n",
    "- NERC (Natural Environment Research Council) \n",
    "- STFC (Science and Technology Facilities Council)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we have seen how to form a small cooccurrence network of research topics, apply community detection and identify clusters of commonly cooccurring topics, which can be considered as an approximation of high level research disciplines.\n",
    "\n",
    "**Where can we go from here?**\n",
    "\n",
    "For a start, cooccurrence networks and community detection do not need to be applied only to topics or keywords. For example, we could use them for people or organisations to study the nature of social networks and collaborations.\n",
    "\n",
    "We can also combine the results from a method such as the one shown here with other techniques, such as supervised machine learning, to create a document labelling algorithm. In this instance, we could train a model to predict discipline labels from project descriptions, and then apply the model to another dataset that does not have research topic, subject or discipline tags. We have used this to transfer discipline labels from Gateway to Research to CORDIS, the European Union's research project database.\n",
    "\n",
    "Or perhaps we might explore the possibilities of creating a more detailed data visualisation, for example one that helps the user to see the hierarchical relationships between topics, subjects and disciplines.\n",
    "\n",
    "How could you apply these methods to your domain or data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Aggregate Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section below shows an example of how to aggregate multiple community detection iterations and draw a network visualisations that has clearer boundaries between communities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregatePartition:\n",
    "    '''AggregatePartition'''\n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph\n",
    "    \n",
    "    def edgelist_to_cooccurrence(self, repeats, **best_partition_kwargs):\n",
    "        edge_counter = Counter()\n",
    "        for i in range(repeats):\n",
    "            partition = community.best_partition(self.graph, random_state=i, **best_partition_kwargs)\n",
    "            edgelist = self.partition_to_edgelist(partition)\n",
    "            edge_counter.update(edgelist)\n",
    "\n",
    "        g = nx.Graph()\n",
    "        g.add_weighted_edges_from([(e[0][0], e[0][1], e[1]) for e in edge_counter.items()])\n",
    "        return g\n",
    "    \n",
    "    def partition_to_edgelist(self, partition):\n",
    "        partition_reverse_mapping = self.reverse_index_partition(partition)\n",
    "        edgelist = []\n",
    "        for community, elements in partition_reverse_mapping.items():\n",
    "            combos = [tuple(sorted(e)) for e in itertools.combinations(elements, 2)]\n",
    "            edgelist.extend(combos)\n",
    "        return edgelist\n",
    "     \n",
    "    def reverse_index_partition(self, partition):\n",
    "        partition_reverse_mapping = defaultdict(list)\n",
    "        for k, v in partition.items():\n",
    "            partition_reverse_mapping[v].append(k)\n",
    "        return partition_reverse_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = AggregatePartition(g)\n",
    "c_co = cp.edgelist_to_cooccurrence(5, resolution=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_c_co = community.best_partition(c_co, resolution=0.4, random_state=42, weight='weight')\n",
    "n_c_co_communities = len(set(part_c_co.values()))\n",
    "print('{} communities detected.'.format(n_c_co_communities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {k: k for k, _ in part.items()}\n",
    "nx.set_node_attributes(c_co, names, name='topic_name')\n",
    "c_co_community_colors = {k: Category20[n_c_co_communities][c] for k, c in part_c_co.items()}\n",
    "nx.set_node_attributes(c_co, c_co_community_colors, name='color')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(c_co, weight='weight', scale=2, seed=42)\n",
    "\n",
    "plot = figure(title=\"Research Topic Cooccurrence Network\",\n",
    "              x_range=(-2.1,2.1), y_range=(-2.1,2.1),\n",
    "             )\n",
    "\n",
    "graph_renderer = from_networkx(c_co, pos, center=(0,0))\n",
    "graph_renderer.node_renderer.glyph = Circle(size=7, fill_color='color', line_color=None)\n",
    "graph_renderer.node_renderer.selection_glyph = Circle(size=7, fill_color='color')\n",
    "graph_renderer.node_renderer.hover_glyph = Circle(size=7, fill_color='color')\n",
    "graph_renderer.node_renderer.muted_glyph = Circle(size=7, fill_color='color', fill_alpha=0.9)\n",
    "\n",
    "\n",
    "graph_renderer.edge_renderer.glyph = MultiLine(line_color=\"#CCCCCC\", line_alpha=0.2, line_width=1)\n",
    "graph_renderer.edge_renderer.selection_glyph = MultiLine(line_color=Spectral4[2], line_width=1.5)\n",
    "graph_renderer.edge_renderer.hover_glyph = MultiLine(line_color=Spectral4[1], line_width=1.5)\n",
    "\n",
    "graph_renderer.selection_policy = NodesAndLinkedEdges()\n",
    "\n",
    "node_hover_tool = HoverTool(tooltips=[(\"Topic\", \"@topic_name\")])\n",
    "plot.add_tools(node_hover_tool, TapTool())\n",
    "\n",
    "plot.renderers.append(graph_renderer)\n",
    "\n",
    "show(plot)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
