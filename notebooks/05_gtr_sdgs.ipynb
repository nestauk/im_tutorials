{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UK Research and the Sustainable Development Goals\n",
    "\n",
    "This tutorial will explore the relationships between the United Nations Sustainable Development Goals (SDGs) and UK publicly funded research projects.\n",
    "\n",
    "The tutorial consists of two segments:\n",
    "1. Constructing a classifier to tag documents with SDG labels using supervised machine learning\n",
    "2. Applying the classifier to UK research projects from Gateway to Research and performing analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# install im_tutorial package\n",
    "!pip install git+https://github.com/nestauk/im_tutorials.git\n",
    "!pip install annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful Python tools\n",
    "from itertools import chain, combinations\n",
    "from collections import Counter\n",
    "\n",
    "# matplotlib for static plots\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "# networkx for networks\n",
    "import networkx as nx\n",
    "# numpy for mathematical functions\n",
    "import numpy as np\n",
    "# pandas for handling tabular data\n",
    "import pandas as pd\n",
    "# seaborn for pretty statistical plots\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('max_columns', 99)\n",
    "\n",
    "# basic bokeh imports for an interactive scatter plot or line chart\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource, Circle, Line\n",
    "\n",
    "# NB: If using Google Colab, this function must be run at \n",
    "# the end of any cell that you want to display a bokeh plot.\n",
    "# If using Jupyter, then this line need only appear once at\n",
    "# the start of the notebook.\n",
    "output_notebook()\n",
    "\n",
    "from im_tutorials.data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sdg = sdg.sdg_web_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_sdg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sdg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_definitions = {\n",
    "     1: '1. No Poverty',\n",
    "     2: '2. Zero Hunger',\n",
    "     3: '3. Good Health & Well-being',\n",
    "     4: '4. Quality Education',\n",
    "     5: '5. Gender Equality',\n",
    "     6: '6. Clean Water & Sanitation',\n",
    "     7: '7. Affordable & Clean Energy',\n",
    "     8: '8. Decent Work & Economic Growth',\n",
    "     9: '9. Industry, Innovation & Infrastructure',\n",
    "     10: '10.  Reduced Inequalities',\n",
    "     11: '11.  Sustainable Cities & Communities',\n",
    "     12: '12.  Responsible Consumption & Production',\n",
    "     13: '13.  Climate Action',\n",
    "     14: '14.  Life Below Water',\n",
    "     15: '15.  Life on Land',\n",
    "     16: '16.  Peace, Justice & Strong Institutions',\n",
    "     17: '17.  Partnerships for the Goals'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_names = list(sdg_definitions.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG Goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sdg['n_goals'] = [len(x) for x in df_sdg['sdg_goals']]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "df_sdg['n_goals'].value_counts().plot.bar(ax=ax)\n",
    "ax.set_title('Number SDGs per Article')\n",
    "ax.set_xlabel('N Goals')\n",
    "ax.set_ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sdg = df_sdg[(df_sdg['n_goals'] > 0) & (df_sdg['n_goals'] < 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_counts = pd.Series(chain(*df_sdg['sdg_goals'])).map(sdg_definitions).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_counts = pd.Series(chain(*df_sdg['sdg_goals'])).map(sdg_definitions).value_counts()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sdg_counts.plot.barh(ax=ax)\n",
    "ax.set_title('Frequency of Goals')\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_ylabel('Goal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sdg = df_sdg[[False if 17 in x else True for x in df_sdg['sdg_goals']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_counts = pd.Series(chain(*df_sdg['sdg_goals'])).map(sdg_definitions).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sdg_counts.plot.barh(ax=ax)\n",
    "ax.set_title('Frequency of Goals')\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_ylabel('Goal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make sure that there is enough text in each article to provide a rich enough source of information for each SDG. We will have a look at the distribution of text lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(df_sdg['text'].str.len(), bins=100)\n",
    "ax.set_title('Text Length')\n",
    "ax.set_xlabel('N Characters')\n",
    "ax.set_ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop any texts that aren't at least the length of an old school tweet (clearly the minimum amount of characters required to convey any meaningful chunk of information in the 21st Century) and any duplicate texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sdg = df_sdg[df_sdg['text'].str.len() > 140]\n",
    "df_sdg = df_sdg.drop_duplicates('text')\n",
    "df_sdg = df_sdg.drop('index', axis=1)\n",
    "df_sdg = df_sdg.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sdg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDG Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation\n",
    "\n",
    "Typically, for computers to understand human language, it needs to be broken down in to components, e.g. sentences, syllables, or words.\n",
    "\n",
    "In the case of this work, we are going to analyse text at the word level. In natural language processing, the componenets below the sentence level are called **tokens**. The process of breaking a piece of text into tokens is called **tokenisation**. A token could be a word, number, email address or punctuation, depending on the exact tokenisation method used.\n",
    "\n",
    "For example, tokenising the  `'The dog chased the cat.'` might give `['The', 'dog', 'chased', 'the', 'cat', '.']`.\n",
    "\n",
    "In this case we will apply some extra processing during the tokenisation phase. We will\n",
    "\n",
    "1. Tokenise each document at the word level.\n",
    "2. Remove punctuation.\n",
    "3. Remove **stop words**, such as `the`, `and`, `to` etc.\n",
    "4. Apply lower case to all tokens.\n",
    "\n",
    "\n",
    "‚ùìWhat are some potential challenges with tokenization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from im_tutorials.features.text_preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [list(chain(*tokenize_document(document))) for document in df_sdg['text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 0\n",
    "n_tokens_print = 10\n",
    "\n",
    "print('Original text of first document:')\n",
    "print(df_sdg['text'].values[0], '\\n')\n",
    "\n",
    "print(f'First {n_tokens_print} tokens in first document {doc_id}:')\n",
    "print(tokenized[doc_id][:n_tokens_print])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many languages, words can have a root which can be modified with suffixes or appendixes or other methods.\n",
    "\n",
    "We can use **lemmatization** to try to extract the root of most words.\n",
    "\n",
    "‚ùìWhy is this useful?\n",
    "\n",
    "‚ùìWhat could go wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "lemmatized = [[wnl.lemmatize(t) for t in b] for b in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 0\n",
    "n_tokens_print = 10\n",
    "\n",
    "print(f'First {n_tokens_print} tokens in first document {doc_id}:')\n",
    "print(tokenized[doc_id][:n_tokens_print], '\\n')\n",
    "\n",
    "print(f'First {n_tokens_print} lemmas in first document {doc_id}:')\n",
    "print(lemmatized[doc_id][:n_tokens_print])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as stop words and punctuation, there may be other words that we want to remove, which are unique to our corpus. Often these are the tokens which appear very often and therefore convey little distinguishing information about each document.\n",
    "\n",
    "Let's count up all of the tokens in our processed corpus and see which are the most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_counts = Counter(chain(*lemmatized))\n",
    "lemma_counts.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removes = ['development', 'country', 'report', 'also', 'action', 'sdg', 'meeting', 'policy', 'including', 'support',\n",
    "          'implementation', 'national', 'new', 'conference', 'government', 'agreement', 'sdgs', 'goal', 'state',\n",
    "          'agenda', 'organization', 'target', 'need', 'system', 'session', 'programme', 'management', 'party',\n",
    "          'event', 'sector', 'process']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_frequency = 5\n",
    "\n",
    "df_sdg['clean_texts'] = [' '.join([t for t in doc if (t not in removes) & (lemma_counts[t] >= min_frequency)]) \n",
    "                     for doc in lemmatized] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 0\n",
    "print(df_sdg['clean_texts'][doc_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Natural to Machine Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have preprocessed our text, we can apply various NLP techniques to further process, analyse, summarise the text, extract information from it, or use it as features in a later analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, when dealing with text, we need to somehow convert it in to numeric data that can be processed and analysed using mathematics. A very simple example would be to count the number of times each token appears in a document. For example if we have the sentence `'I like really cute cats, but all cats are cute really.'`, after pre-processing and tokenisation, we could generate a vector of word counts where each position represents the token count:\n",
    "\n",
    "```\n",
    "vector      token\n",
    "[1,         i\n",
    " 1,         like\n",
    " 2,         really\n",
    " 2,         cute\n",
    " 2,         cats\n",
    " 1,         but\n",
    " 1,         all\n",
    " 1,]        are\n",
    "```\n",
    "\n",
    "This method is called the **bag of words** approach, and in this case we can determine that the document is about really cute cats. But in real life, with many documents, things are not always so straightfoward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "bow_vecs = count_vectorizer.fit_transform(df_sdg['clean_texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 0\n",
    "n_top_terms = 10\n",
    "\n",
    "def get_vec_counts(bow, idx):\n",
    "    return np.array(bow.todense()[idx])[0]\n",
    "\n",
    "def get_top_terms(bow, doc_id):\n",
    "    vec_counts = get_vec_counts(bow, doc_id)\n",
    "    topn = np.argsort(vec_counts)[::-1][:n_top_terms]\n",
    "    top_counts = vec_counts[topn]\n",
    "    top_terms = vocab[topn]\n",
    "    return top_terms, top_counts\n",
    "\n",
    "top_terms, top_counts = get_top_terms(bow_vecs, doc_id)\n",
    "\n",
    "for term, count in zip(top_terms, top_counts):\n",
    "    print(count, term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìWhat are some potential limitations of bag of words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf-Idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An improvement on the simple bag of words is to somehow weight each token by it's importance, or how much information it carries. One way to to do this is by weighting the count of each word in a document with the inverse of its frequency across _all_ documents. This is called **term frequency-inverse document frequency** or **tf-idf**.\n",
    "\n",
    "By doing this, a reasonably common word like `'height'` would probably be weighted lower than a less common, but more specific term such as `'altitude'`. Even if we have a document where height is mentioned more frequently than altitude, tf-idf can help us to identify that the document is referring to height in the context of altitude, rather than for example the height of a person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "tfidf_vecs = tfidf.fit_transform(bow_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 0\n",
    "n_top_terms = 10\n",
    "\n",
    "top_terms, top_counts = get_top_terms(tfidf_vecs, doc_id)\n",
    "\n",
    "print('Score Term')\n",
    "for term, count in zip(top_terms, top_counts):\n",
    "    print(f'{count:.3f}', term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that terms that are much more specific are weighted relatively higher than those which convey higher level and more generic information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our documents described as vectors, we can visualise them!\n",
    "\n",
    "To do this, we will need to project the high-dimensional document vectors in to low-dimensional space, in this case 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD takes our sparse tf-idf vectors and compresses them in to a lower dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=30)\n",
    "svd_vecs = svd.fit_transform(tfidf_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSNE does a further projection in to 2 dimensional space. It tries to strike a balance between retaining local and global structure, making it good for visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "tsne_vecs = tsne.fit_transform(svd_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_goals = (df_sdg['n_goals'] == 1).index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_vecs_single = tsne_vecs[single_goals]\n",
    "goal_labels_single = [g[0] for g in df_sdg['sdg_goals'][single_goals]]\n",
    "titles_single = df_sdg['title'][single_goals].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import HoverTool\n",
    "from bokeh.palettes import Category20_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [Category20_16[g-1] for g in goal_labels_single]\n",
    "\n",
    "cds = ColumnDataSource(data={\n",
    "    'tsne_0': tsne_vecs[:, 0],\n",
    "    'tsne_1': tsne_vecs[:, 1],\n",
    "    'color': colors,\n",
    "    'goal': [sdg_definitions[g] for g in goal_labels_single],\n",
    "    'title': titles_single\n",
    "})\n",
    "\n",
    "p = figure(width=900, title='TSNE Plot of Single SDG Article Vectors')\n",
    "\n",
    "hover = HoverTool(tooltips=[('Goal', '@goal'), ('Title', '@title')])\n",
    "\n",
    "p.circle(source=cds, x='tsne_0', y='tsne_1', color='color', line_width=0, legend='goal', radius=0.4, alpha=0.9)\n",
    "p.add_tools(hover)\n",
    "\n",
    "show(p)\n",
    "# output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìDoes this visualisation have implications for our classification problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have preprocessed and seen how to vectorized our text, it is time to build and train our model.\n",
    "\n",
    "In this case, our **features** are the vectors created from the documents and our **target labels** are the SDG goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a **multi-class** **multi-label** classification problem. This means that we have more than two possible labels - one for each of the SDGs - and each document can have more than one label assigned to it.\n",
    "\n",
    "To train a model that can deal with this situation, we first need to transform our labels into binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_classes = list(range(1, 17))\n",
    "mlb = MultiLabelBinarizer(classes=sdg_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_goals_mlb = pd.DataFrame(mlb.fit_transform(df_sdg['sdg_goals']), columns=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_goals_mlb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count up the goals again just to make sure that we've done the right thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_goals_mlb.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks ok!\n",
    "\n",
    "Now we can also look to see whether there are any patterns between the SDGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(sdg_goals_mlb.corr(),ax=ax)\n",
    "ax.set_title('Correlation Between SDG Labels');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the accuracy of our model, we will need to test it on data that is also labelled, but that it has not been trained with.\n",
    "\n",
    "As we do not have a separate test dataset for this, we will hold back some of the original data with a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to hold back 20% of our data for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vecs_train, tfidf_vecs_test, sdg_labels_train, sdg_labels_test = train_test_split(\n",
    "    tfidf_vecs, sdg_goals_mlb, test_size=0.2\n",
    ")\n",
    "\n",
    "print('Training set length:', tfidf_vecs_train.shape[0])\n",
    "print('Test set length:', tfidf_vecs_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Quick Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "knc = KNeighborsClassifier()\n",
    "knc.fit(tfidf_vecs_train, sdg_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = knc.predict(tfidf_vecs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(sdg_labels_test, preds, target_names=sdg_names[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great. It's not optimised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Real Thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to train separate models for each SDG that will each be tuned with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.metrics import classification_report_imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline_imb(\n",
    "    RandomUnderSampler(), \n",
    "    LogisticRegression(solver='lbfgs', fit_intercept=False)\n",
    ")\n",
    "\n",
    "C = np.logspace(-1, 2, 10)\n",
    "strats = [0.5, 0.6, 0.7, 0.8]\n",
    "\n",
    "params = {\n",
    "    'randomundersampler__sampling_strategy': strats,\n",
    "    'logisticregression__C': C,\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=params, cv=KFold(n_splits=3, shuffle=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make all of the same vectorisation techniques we did before. We are also going to use SVD to create dense vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfv = TfidfVectorizer(ngram_range=(1,2))\n",
    "tfidf_vecs = tfidfv.fit_transform(df_sdg['clean_texts'])\n",
    "\n",
    "svdv = TruncatedSVD(n_components=300)\n",
    "svd_vecs = svdv.fit_transform(tfidf_vecs)\n",
    "\n",
    "svd_vecs_train, svd_vecs_test, sdg_labels_train, sdg_labels_test = train_test_split(\n",
    "    svd_vecs, sdg_goals_mlb, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {}\n",
    "preds = {}\n",
    "\n",
    "for i in range(1, 17):\n",
    "    print(sdg_definitions[i])\n",
    "    grid.fit(svd_vecs_train, sdg_labels_train[i])\n",
    "    best = grid.best_estimator_\n",
    "    classifiers[i] = best\n",
    "    preds[i] = best.predict(svd_vecs_test)\n",
    "    print(classification_report_imbalanced(sdg_labels_test[i], preds[i]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìWhat might be limiting the accuracy of these classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gateway to Research Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gateway to Research is a database of UK funded research projects across all disciplines. This sample of their database contains titles, abstracts, research categories and the start year for each project.\n",
    "\n",
    "We are going to apply our SDG classifier to the abstracts and then have a look at some possible analysis methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gtr = datasets.gateway_to_research_projects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gtr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(df_gtr['abstract_texts'].str.len(), bins=100)\n",
    "ax.set_title('Abstract Length Histogram')\n",
    "ax.set_xlabel('N Characters')\n",
    "ax.set_ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gtr['abstract_texts'].value_counts()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gtr.groupby('start_year')['project_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gtr = df_gtr[(df_gtr['start_year'] > 2005) & (df_gtr['start_year'] < 2018)]\n",
    "text_drop = df_gtr['abstract_texts'].value_counts().index[0]\n",
    "df_gtr = df_gtr[~pd.isnull(df_gtr['abstract_texts'])]\n",
    "df_gtr = df_gtr[df_gtr['abstract_texts'].str.len() > 140]\n",
    "df_gtr = df_gtr[df_gtr['abstract_texts'] != text_drop]\n",
    "df_gtr = df_gtr.sort_values('start_year')\n",
    "df_gtr = df_gtr.reset_index()\n",
    "df_gtr = df_gtr.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gtr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_gtr = [list(chain(*tokenize_document(document))) for document in df_gtr['abstract_texts'].values]\n",
    "lemmatized_gtr = [[wnl.lemmatize(t) for t in b] for b in tokenized_gtr]\n",
    "df_gtr['clean_texts'] = [' '.join(t) for t in lemmatized_gtr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_gtr = tfidfv.transform(df_gtr['clean_texts'])\n",
    "svd_gtr = svdv.transform(tfidf_gtr)\n",
    "sdgs_gtr = {}\n",
    "for i, clf in classifiers.items():\n",
    "    sdgs_gtr[i] = clf.predict(svd_gtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gtr_sdgs = pd.DataFrame(sdgs_gtr)\n",
    "df_gtr_sdgs.columns = [sdg_definitions[i] for i in df_gtr_sdgs.columns]\n",
    "df_gtr_sdgs.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count = Counter(chain(*df_gtr['research_topics']))\n",
    "subject_count = Counter(chain(*df_gtr['research_subjects']))\n",
    "print('N Topics:', len(topic_count))\n",
    "print('N Subjects:', len(subject_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = sorted(set(chain(*df_gtr['research_subjects'])))\n",
    "mlb_subjects = MultiLabelBinarizer(classes=rs)\n",
    "subjects_mlb_df = mlb_subjects.fit_transform(df_gtr['research_subjects'])\n",
    "subjects_mlb_df = pd.DataFrame(subjects_mlb_df, columns=rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "sns.heatmap((df_gtr_sdgs.groupby(df_gtr['funder_name']).sum() / \n",
    " df_gtr_sdgs.groupby(df_gtr['funder_name']).sum().sum() * 100).T, ax=ax,\n",
    "           annot=True, fmt='.1f')\n",
    "ax.set_title('Percentage Projects by Funder for Each SDG')\n",
    "ax.set_xlabel('Funder Name')\n",
    "ax.set_ylabel('Goal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation With Research Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_subj_corrs = np.zeros((subjects_mlb_df.shape[1], len(sdg_names) -1))\n",
    "\n",
    "for i, subj in enumerate(subjects_mlb_df.columns):\n",
    "    for j, sdg in enumerate(df_gtr_sdgs.columns):\n",
    "        corr = pearsonr(subjects_mlb_df[subj], df_gtr_sdgs[sdg])[0]\n",
    "        sdg_subj_corrs[i, j] = corr\n",
    "        \n",
    "sdg_subj_corrs_df = pd.DataFrame(sdg_subj_corrs,\n",
    "                                 columns=df_gtr_sdgs.columns,\n",
    "                                 index=subjects_mlb_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "sns.heatmap(sdg_subj_corrs_df.T, ax=ax, cmap='viridis')\n",
    "ax.set_title('Pearson R Correlation Coefficient between SDGs and Research Subjects')\n",
    "ax.set_xlabel('SDG')\n",
    "ax.set_ylabel('Research Subject');\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDGs Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window = 3\n",
    "sdgs_time_df = (\n",
    "    (df_gtr_sdgs.groupby(df_gtr['start_year']).sum().loc[2006:2017].divide( \n",
    "     df_gtr.groupby('start_year')['project_id'].count().loc[2006:2017].values, axis=0) * 100)\n",
    "     .rolling(rolling_window).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = figure(title='Percentage of GtR Projects by SDG over Time', width=900, height=500)\n",
    "\n",
    "for i, col in enumerate(sdgs_time_df.columns):\n",
    "    color = Category20_16[i-1]\n",
    "    p2.line(\n",
    "        x=sdgs_time_df.index.values, \n",
    "        y=sdgs_time_df[col],\n",
    "        color=color,\n",
    "        line_width=3,\n",
    "        alpha=0.8,\n",
    "        legend=col,\n",
    "        muted_color=color,\n",
    "        muted_alpha=0.3\n",
    "    )\n",
    "\n",
    "p2.legend.location = \"top_left\"\n",
    "p2.legend.click_policy=\"mute\"\n",
    "p2.legend.label_text_font_size = '6pt'\n",
    "\n",
    "show(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìWhat are the caveats behind this plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes in Term Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our projects classified, we can hone in on individual SDGs.\n",
    "\n",
    "Here's a simple example where we look at which new terms are highly correlating each year with SDG 7. Affordable & Clean Energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_range = (1, 1)\n",
    "\n",
    "found = set()\n",
    "for year, group in df_gtr.groupby('start_year'):\n",
    "    print('===', year, '===')\n",
    "    tfidf_corr = TfidfVectorizer(ngram_range=ngram_range)\n",
    "    tfidf_corr_vecs = tfidf_corr.fit_transform(group['clean_texts'])\n",
    "    \n",
    "    ids = group.index.values\n",
    "    sdg_labels = df_gtr_sdgs[sdg_names[6]].iloc[ids]\n",
    "    skb = SelectKBest(k=20)\n",
    "    vocab_corr = {v: k for k, v in tfidf_corr.vocabulary_.items()}\n",
    "    skb.fit(tfidf_corr_vecs, df_gtr_sdgs[sdg_names[6]].values[ids])\n",
    "    top_term_ids = np.argsort(np.nan_to_num(skb.scores_, 0))[::-1][0:50]\n",
    "    top_terms = [vocab_corr[v] for v in top_term_ids]\n",
    "    not_found_before = [t for t in top_terms if t not in found]\n",
    "    found.update(top_terms)\n",
    "    print(not_found_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìHow could we improve on this method to find the most emerging terms associated with the SDG?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An Aside on N Grams\n",
    "\n",
    "We know that some in some cases, we might have words that appear together more often than we might expect. This might happen where we have commonly used phrases, or names of entities, for example `general relativity`. It can be useful to identify cases of this in our text so that the machine can understand that they represent different information when compared to the words appearing separately. Tokens of multiple words are called **n grams**. N grams containing two tokens are **bigrams**, n grams containing three words are **trigrams** and so on.\n",
    "\n",
    "For example, in a corpus of text, we might have the sentence, `'I travelled from York to New York to find a new life.'`. After tokenisation and finding bigrams, we might end up with `['i', 'travelled', 'from', 'york', 'to', 'new_york', 'to', 'find', 'a', 'new', 'life', '.']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics for Renewable Energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modelling\n",
    "\n",
    "When we have thousands of documents, it can be too many for a single person to read and understand in a reasonable space of time. A useful first step is often to be able to understand what the main themes are within the documents we have. Bag of words or tf-idf are useful processing methods, but they still require us to inspect each document individually or group them and identify topics manually. \n",
    "\n",
    "Luckily, there are automated methods of finding the groups of tokens that describe broad themes within a set of documents, which are referred to as **topic modelling**.\n",
    "\n",
    "In this case, we are going to use **Latent Semantic Indexing** or **LSI**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sdg_7 = df_gtr[(df_gtr_sdgs[sdg_names[6]] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from gensim.sklearn_api.lsimodel import LsiTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_7_tokenised = [list(chain(*tokenize_document(document))) for document in df_sdg_7['clean_texts'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(sdg_7_tokenised)\n",
    "dictionary.filter_extremes(no_above=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_sdg_7 = [dictionary.doc2bow(d) for d in sdg_7_tokenised]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 300\n",
    "lsi = LsiModel(bow_sdg_7, id2word=dictionary, num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics_print = 10\n",
    "\n",
    "for topic_id in range(0, num_topics, int(num_topics/n_topics_print)):\n",
    "    print('Topic', topic_id)\n",
    "    print(lsi.print_topic(topic_id), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìDo these topics reveal anything to you about the challenges of using machine learning methods exploring research topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_7 = TruncatedSVD(n_components=30)\n",
    "svd_7_vecs = svd_7.fit_transform(lsi_vecs)\n",
    "tsne_7 = TSNE(n_components=2)\n",
    "tsne_7_vecs = tsne_7.fit_transform(svd_7_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agc = AgglomerativeClustering(n_clusters=20, affinity='cosine', linkage='complete')\n",
    "agcs = agc.fit_predict(svd_7_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topic_terms(model, num_topic_terms):\n",
    "    topic_terms = []\n",
    "    for i in range(model.num_topics):\n",
    "        topic_terms.append([t[0] for t \n",
    "                   in model.show_topic(i)[:num_topic_terms]])        \n",
    "    return np.array(topic_terms)\n",
    "\n",
    "def make_topic_names(topic_vectors, topic_terms, num_topics=None):\n",
    "    topic_names = []\n",
    "    for vector in topic_vectors:\n",
    "        topic_ids = np.argsort(vector)[::-1][:num_topics]\n",
    "        name = ', '.join([c for c in chain(*topic_terms[topic_ids])])\n",
    "        topic_names.append(name)\n",
    "    return topic_names\n",
    "\n",
    "topic_terms = make_topic_terms(lsi, 2)\n",
    "\n",
    "topic_names = make_topic_names(lsi_vecs, topic_terms, num_topics=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [x[:50] for x in df_gtr.iloc[df_sdg_7.index.values]['clean_texts'].values]\n",
    "\n",
    "cmap = matplotlib.cm.hsv\n",
    "norm = matplotlib.colors.Normalize(vmin=np.min(agcs), vmax=np.max(agcs))\n",
    "colors = [matplotlib.cm.colors.to_hex(cmap(norm(i))) for i in agcs]\n",
    "\n",
    "cds = ColumnDataSource(data={'tsne_0': tsne_7_vecs[:, 0],\n",
    "                             'tsne_1': tsne_7_vecs[:, 1],\n",
    "                             'name': titles,\n",
    "                             'color': colors,\n",
    "                             'cluster': agcs})\n",
    "\n",
    "p = figure(width=900)\n",
    "hover = HoverTool(tooltips=[(\"Topic\", \"@name\"), (\"Cluster\", \"@cluster\")])\n",
    "p.circle(source=cds, x='tsne_0', y='tsne_1', fill_color='color', line_color='color', \n",
    "         fill_alpha=0.5, line_alpha=0.5, radius=.5)\n",
    "p.add_tools(hover)\n",
    "\n",
    "show(p)\n",
    "\n",
    "# output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Communities of Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our topic modeled projects can now be projected in to space, and we can find nearnest neighbours using Cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sdg_7 = df_sdg_7.reset_index()\n",
    "df_sdg_7 = df_sdg_7.drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annoy_indices = {}\n",
    "for year, group in df_sdg_7.groupby(['start_year']):\n",
    "    ids = group.index.values\n",
    "\n",
    "    vecs = svd_7_vecs[ids]\n",
    "    t = AnnoyIndex(svd_7.n_components, 'angular')  # Length of item vector that will be indexed\n",
    "    for idx, vec in zip(ids, vecs):\n",
    "        t.add_item(idx, vec)\n",
    "    t.build(500)\n",
    "    annoy_indices[year] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = df_sdg_7['start_year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dist = 0.8\n",
    "\n",
    "project_edges = defaultdict(list)\n",
    "\n",
    "for year, group in df_sdg_7.groupby(['start_year']):\n",
    "    edges_year = []\n",
    "    ids = group.index.values\n",
    "    annoy_index = annoy_indices[year]\n",
    "    for idx in ids:\n",
    "        for neighbour_idx in annoy_index.get_nns_by_item(idx, 30):\n",
    "            if neighbour_idx == idx:\n",
    "                continue\n",
    "            else:\n",
    "                dist = annoy_index.get_distance(neighbour_idx, idx)\n",
    "                if dist < min_dist:\n",
    "                    edges_year.append((idx, neighbour_idx, {'dist': 1 - dist}))\n",
    "    project_edges[year].extend(edges_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_p = nx.Graph()\n",
    "g_p.add_edges_from(project_edges[2007])\n",
    "\n",
    "g_p_node_pos = nx.spring_layout(g_p, seed=101, weight='dist')\n",
    "nx.draw(g_p, pos=g_p_node_pos, node_size=15, node_color='C0');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = community.best_partition(g_p, resolution=0.3, weight='dist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(g_p, pos=g_p_node_pos, node_size=15, node_color=list(communities.values()), cmap=matplotlib.cm.hsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 0.3\n",
    "\n",
    "project_communities = {}\n",
    "community_labels = {}\n",
    "project_graphs = {}\n",
    "for year, edge_list in project_edges.items():\n",
    "    g = nx.Graph()\n",
    "    g.add_edges_from(edge_list)\n",
    "    project_graphs[year] = g\n",
    "    \n",
    "    communities = community.best_partition(g, resolution=resolution, weight='dist')\n",
    "    print(f'N Communities at {year}:', len(set(communities.values())))\n",
    "    \n",
    "    community_ids = defaultdict(list)\n",
    "    for proj, c in communities.items():\n",
    "        community_ids[c].append(proj)\n",
    "    project_communities[year] = community_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_communities = {}\n",
    "\n",
    "for year, communities_year in project_communities.items():\n",
    "    svd_communities_year = []\n",
    "    for community_id, docs in communities_year.items():\n",
    "        mean_vec = np.mean(svd_7_vecs[docs], axis=0)\n",
    "        mean_vec = mean_vec / np.max(mean_vec)\n",
    "        svd_communities_year.append(mean_vec)\n",
    "    svd_communities[year] = svd_communities_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_thresh = 0.5\n",
    "\n",
    "agg_edges = []\n",
    "max_parents = 1\n",
    "\n",
    "for i, year in enumerate(sorted(years)):\n",
    "    if i > 0:\n",
    "        past_year = year - 1\n",
    "        past_vecs = svd_communities[past_year]\n",
    "        current_vecs = svd_communities[year]\n",
    "        for idx, vec in enumerate(current_vecs):\n",
    "            similarities = [1 - cosine(vec, c_past) for c_past in past_vecs]\n",
    "            sim_max_ids = np.argsort(similarities)[::-1][:max_parents]\n",
    "            for sim_max_idx in sim_max_ids:\n",
    "                edge = (f'{year}_{idx}', f'{past_year}_{sim_max_idx}', {'weight': similarities[sim_max_idx]})\n",
    "            agg_edges.append(edge)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "for year, communities in project_communities.items():\n",
    "    for idx, _ in enumerate(communities):\n",
    "        nodes.append(f'{year}_{idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([e[2]['weight'] for e in agg_edges], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = nx.DiGraph()\n",
    "h.add_nodes_from(nodes)\n",
    "h.add_edges_from(agg_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_x = np.array([int(d.split('_')[0]) for d in h.nodes])\n",
    "pos_x = pos_x - np.max(pos_x)\n",
    "\n",
    "tsne_agg = TSNE(n_components=1)\n",
    "svd_df = pd.DataFrame(np.array(list(chain(*svd_communities.values()))))\n",
    "pos_y = tsne_agg.fit_transform(svd_df)\n",
    "\n",
    "pos_y = pos_y - np.min(pos_y) \n",
    "pos_y = pos_y / np.max(pos_y)\n",
    "\n",
    "pos = {}\n",
    "for node, x, y in zip(h.nodes, pos_x, pos_y):\n",
    "    pos[node] = (x, y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([1 / h.get_edge_data(e[0], e[1])['weight'] for e in h.edges])\n",
    "weights = weights / np.max(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = int(np.round(np.mean([len(c) for c in svd_communities.values()])))\n",
    "\n",
    "km = KMeans(n_clusters=n_clusters)\n",
    "km.fit(list(chain(*svd_communities.values())))\n",
    "colors = km.labels_\n",
    "cmap_nodes = matplotlib.cm.hsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = matplotlib.cm.get_cmap('inferno')\n",
    "fig, ax = plt.subplots(figsize=(15, 7.5))\n",
    "nx.draw(h, pos=pos, node_size=50, edge_color=weights, edge_cmap=cmap, width=2, node_color=colors, cmap=cmap_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 3\n",
    "\n",
    "lsi_years = {}\n",
    "\n",
    "for year, group in df_sdg_7.groupby(['start_year']):\n",
    "    ids = group.index.values\n",
    "    lsi_year = np.sqrt(np.square(lsi_vecs[ids]))\n",
    "    lsi_mean = np.mean(lsi_year, axis=0)\n",
    "    lsi_years[year] = lsi_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_df = pd.DataFrame(lsi_years).T.rolling(3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# megs = lsi_df.multiply(df_sdg_7.groupby('start_year')['project_id'].count(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "\n",
    "fig, axs = plt.subplots(nrows=n_topics, figsize=(6, 1.2 * n_topics))\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.plot(lsi_df[i], linewidth=2)\n",
    "    title = ' '.join([c[0] for c in lsi.show_topic(i)][:5])\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_edges = []\n",
    "for i, vec in enumerate(lsi_vecs):\n",
    "    top_topics = np.argsort(vec)[::-1][:top_n]\n",
    "    for combo in combinations(top_topics, 2):\n",
    "        co_edges.append(tuple(sorted(combo)))\n",
    "co_edges = list(set(co_edges))\n",
    "\n",
    "g_topic_co = nx.Graph()\n",
    "g_topic_co.add_edges_from(co_edges)\n",
    "\n",
    "b = np.array(list(nx.centrality.betweenness_centrality(g_topic_co)))\n",
    "d = np.array(list(nx.centrality.degree_centrality(g_topic_co).values())) * 100\n",
    "\n",
    "cmap = matplotlib.cm.inferno\n",
    "norm = matplotlib.colors.Normalize(vmin=np.min(b), vmax=np.max(b))\n",
    "colors = [cmap(norm(i)) for i in b]\n",
    "nx.draw(g_topic_co, node_size=d, node_color=colors, edge_color='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
