{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Innovation Mapping with Python\n",
    "\n",
    "Bring some life to your innovation mapping notebook analysis with interactive data viz! ðŸ•¹\n",
    "\n",
    "---\n",
    "\n",
    "This tutorial covers a few examples of interactive data visualisation with Python that can be used to create rich notebooks or prototypes for web visualisations.\n",
    "\n",
    "In this tutorial, we are going to be based on Bokeh and will make use of HoloViews, GeoViews and Datashader. There are many options for interactive data visualisation with Python however, including Altair, Plotly, Dash, and even Matplotlib, so try them out too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# install im_tutorial package\n",
    "# !pip install git+https://github.com/nestauk/im_tutorials.git\n",
    "# !pip install python-louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful Python tools\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "# matplotlib for static plots\n",
    "import matplotlib.pyplot as plt\n",
    "# networkx for networks\n",
    "import networkx as nx\n",
    "# numpy for mathematical functions\n",
    "import numpy as np\n",
    "# pandas for handling tabular data\n",
    "import pandas as pd\n",
    "# seaborn for pretty statistical plots\n",
    "import seaborn as sns\n",
    "\n",
    "from im_tutorials.data.gtr import gtr_table, gtr_link_table, gtr_table_list\n",
    "\n",
    "pd.set_option('max_columns', 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GtR Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_df = gtr_table('projects')\n",
    "gtr_funds_df = gtr_table('funds')\n",
    "gtr_funds_link_table = gtr_link_table('funds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Join funding table to link table to get project ids. Groupby project to get start and end date, sum of funding.\n",
    "- Group leads and collaborators and create network\n",
    "- Join with project descriptions and make collaboration network with SDGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_funds_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_funds_link_table.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_funds_df = gtr_funds_df.merge(gtr_funds_link_table, left_on='id', right_on='id')\n",
    "gtr_funds_df = gtr_funds_df.drop_duplicates(['project_id', 'amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_funds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Earliest start date:', gtr_funds_df['start'].min())\n",
    "print('Earliest end date:', gtr_funds_df['end'].min())\n",
    "print('\\n')\n",
    "print('Latest start date:', gtr_funds_df['start'].max())\n",
    "print('Latest end date:', gtr_funds_df['end'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_funds_df['start'].dt.year.value_counts()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_funds_df['end'].dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_start_year = 2006\n",
    "max_start_year = 2019\n",
    "max_end_year = 2030\n",
    "\n",
    "gtr_funds_df = gtr_funds_df[(gtr_funds_df['start'].dt.year >= min_start_year) & \n",
    "                            (gtr_funds_df['start'].dt.year < max_end_year)]\n",
    "gtr_funds_df = gtr_funds_df[(gtr_funds_df['end'].dt.year <= max_end_year)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_projects_funds_df = gtr_projects_df.merge(\n",
    "    gtr_funds_df, left_on='id', right_on='project_id', \n",
    "    how='left', suffixes=('_proj', '_fund'))\n",
    "\n",
    "gtr_project_funds_df = gtr_projects_funds_df.drop_duplicates(subset=['project_id'])\n",
    "gtr_project_funds_df['start_year'] = gtr_project_funds_df['start_fund'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column that just has the start year\n",
    "gtr_project_funds_df['start_year'] = gtr_project_funds_df['start_fund'].dt.year\n",
    "# group funds by start of year and funder\n",
    "amount_year_sum = gtr_project_funds_df.groupby(['start_year', 'leadFunder'])['amount'].sum()\n",
    "amount_year_sum = amount_year_sum.loc[2006:2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_year_sum.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_year_sum = amount_year_sum.unstack()\n",
    "amount_year_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window = 3\n",
    "amount_year_sum_rolling = amount_year_sum.rolling(rolling_window).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Usual Suspect\n",
    "\n",
    "**Matplotlib** is a very popular library for plotting in Python. It can be used to produce publication worthy plots, but is typically restricted to producing static images.\n",
    "\n",
    "Here we will plot the total amounts awarded by each funder in each year, with a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(amount_year_sum_rolling, marker='o')\n",
    "ax.legend(amount_year_sum_rolling.columns)\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Total Funding (Â£)')\n",
    "ax.set_title('Total Funding Over Time');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Level\n",
    "\n",
    "Matplotlib is great for scientific plots, but for interaction we need to look elsewhere. Here we are going to try **Bokeh** to produce an interactive version of exactly the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need these to make plot visible in notebook\n",
    "from bokeh.io import show, output_notebook\n",
    "# figure object to \n",
    "from bokeh.plotting import figure\n",
    "# basic bokeh building blocks\n",
    "from bokeh.models import ColumnDataSource, Circle, Line\n",
    "from bokeh.models import PrintfTickFormatter, HoverTool\n",
    "from bokeh.palettes import Category10\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_funders = len(amount_year_sum_rolling.columns)\n",
    "cmap = Category10[n_funders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a figure object with desired dimensions\n",
    "p = figure(width=550, height=350,\n",
    "           title='Total Awards by Funder over Time')\n",
    "\n",
    "# loop through columns, select color, plot line and circles\n",
    "for i, c in enumerate(amount_year_sum.columns):\n",
    "    color = cmap[i]\n",
    "    p.line(\n",
    "        x=amount_year_sum_rolling.index.values, \n",
    "        y=amount_year_sum_rolling[c], \n",
    "        legend=c,\n",
    "        color=color,\n",
    "        line_width=2,\n",
    "        alpha=0.7,\n",
    "        name=c,\n",
    "        muted_alpha=0.1,\n",
    "        muted_color=color\n",
    "          )\n",
    "    p.circle(\n",
    "        x=amount_year_sum_rolling.index.values, \n",
    "        y=amount_year_sum_rolling[c], \n",
    "        legend=c,\n",
    "        color=color, \n",
    "        name=c,\n",
    "        muted_alpha=0.1, \n",
    "        muted_color=color\n",
    "    )\n",
    "\n",
    "# build a hover tool that will display funding amount (y value), \n",
    "# year (x value) and funding amount\n",
    "hover = HoverTool(tooltips=[('Amount', 'Â£@y{( 0.00 a)}'),\n",
    "                            ('Year', '@x{%F}'),\n",
    "                            ('Funder', '$name')],\n",
    "                 )\n",
    "p.add_tools(hover)\n",
    "\n",
    "# add labels and formatting\n",
    "p.xaxis.axis_label = 'Year'\n",
    "p.yaxis.axis_label = 'Total Funding'    \n",
    "p.yaxis[0].formatter = PrintfTickFormatter(format=\"Â£%.1e\")\n",
    "# add interactive legend\n",
    "p.legend.click_policy = \"mute\"\n",
    "p.legend.location = 'top_left'\n",
    "p.legend.label_text_font_size = '6pt'\n",
    "    \n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bokeh Scatter (Circle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use Bokeh to make simple interactive plots such as a scatter diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = (gtr_funds_df['end'] - gtr_funds_df['start']).dt.days / 365.25\n",
    "amount = gtr_funds_df['amount']\n",
    "\n",
    "p = figure(width=550, height=350, y_axis_type=\"log\")\n",
    "p.grid.visible = False\n",
    "\n",
    "p.circle(x=duration, y=amount, size=1, alpha=0.05)\n",
    "\n",
    "p.xaxis.axis_label = 'Duration (years)'\n",
    "p.yaxis.axis_label = 'Funding Amount (Â£)'\n",
    "\n",
    "p.add_tools(HoverTool(\n",
    "    tooltips=[(\"Duration\", \"$x\"), (\"Amount\", \"$y\")],\n",
    "    mode=\"mouse\", point_policy=\"follow_mouse\"\n",
    "))\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declarative Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is it?\n",
    "\n",
    "> By declarative, we mean that while plotting any chart, you only need to declare links between data columns to the encoding channels, such as x-axis, y-axis, colour, etc. and rest all of the plot details are handled automatically. Letâ€™s understand it by an example. \n",
    "\n",
    "[https://www.analyticsvidhya.com/blog/2017/12/introduction-to-altair-a-declarative-visualization-in-python/](https://www.analyticsvidhya.com/blog/2017/12/introduction-to-altair-a-declarative-visualization-in-python/)\n",
    "\n",
    "Instead of having to faff around with lots of details, declarative plotting libraries do lots of the work behing the scenes and are loaded with good default settings. These can normally be overridden by just changing some options, making the code a lot cleaner, and the workflow a lot faster for exploratory analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declarative Hexbin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our declarative data visualisation, we are going to use **Holoviews**. Holoviews is a library that simply puts your data, the relationships between the data and the plot, and any plot settings you want to apply in to a structure that is ready to be plotted. \n",
    "\n",
    "It can't actually do the plotting on its own, so we need to specify a \"back end\" to do the work. Because of this, Holoviews can actually be initialised with different plotting libraries as the back end. In this case we will use Bokeh as we want to retain the interactivity, but we could also have used Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = amount > 0\n",
    "df_hex = pd.DataFrame({'Project Duration (years)': duration,\n",
    "                   'Funding Amount': np.log10(amount[mask])})\n",
    "\n",
    "hx = hv.HexTiles(df_hex)\n",
    "hx.opts(width=550, height=350, logz=True, yformatter='Â£10^%d',\n",
    "        tools=['hover'], hover_color='pink', hover_alpha=0.7,\n",
    "        title='Funding Amount by Project Duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the plotting options for a specific Holoviews chart type, you can interrogate the object with `?` notation.\n",
    "\n",
    "```python\n",
    "from holoviews import opts\n",
    "opts.HexTiles?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're at it, let's see what it would have looked like to make the scatter plot above with Holoviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scat = hv.Scatter(df_hex.sample(frac=0.1))\n",
    "scat.opts(width=550, height=350, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holoviews is also great at handling layouts. Let's overlay a scatter on top of the hexbin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scat = hv.Scatter(df_hex.sample(frac=0.05))\n",
    "scat.opts(alpha=0.3, color='white', size=0.5)\n",
    "\n",
    "hx * scat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular type of interactive chart is a map, due to the ability to zoom and pan. This makes sense as large geographic datasets can have both high level and low level structure.\n",
    "\n",
    "Let's pull the details of the organisations in Gateway to Research and explore their locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_org_locs_df = gtr_table('organisations_locations')\n",
    "gtr_org_locs_df.head()\n",
    "\n",
    "# drop orgs where no location is available\n",
    "gtr_org_locs_df = gtr_org_locs_df[(~pd.isnull(gtr_org_locs_df['latitude'])) &\n",
    "                                  (~pd.isnull(gtr_org_locs_df)['longitude'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Folium** is a Python wrapper around the web visualisation mapping library Leaflet.js. It can be easily used to produce familiar looking visualisations, using openly available map tile sets. Let's use it to make a cluster marker style plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from im_tutorials.data.gis import country_basic_info\n",
    "country_df = country_basic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import MarkerCluster, FastMarkerCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_c, lng_c = country_df.set_index('alpha3Code').loc['GBR']['latlng']\n",
    "\n",
    "cluster_map = folium.Map(location=[lat_c, lng_c], zoom_start=5, width=550, height=550)\n",
    "cluster_map.add_child(FastMarkerCluster(data=gtr_org_locs_df[['latitude', 'longitude']].values.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folium can also be used to plot vectors, rasters and choropleths.\n",
    "\n",
    "**Tasks**\n",
    "\n",
    "1. Change the marker icon.\n",
    "2. Remove the organisations at duplicate addresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datashader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does one job. Does it well.\n",
    "\n",
    "Sometimes, you may have a lot of points to plot. We saw earlier with Bokeh that it took a while to plot a few thousand scatter points. We also saw that the hexbin did a much better job of showing the information at a low zoom level. Datashader solves both of these problems.\n",
    "\n",
    "Datashader takes 2 dimensional point data and rapidly turns it into an intesnity map image. Let's try it on the same data as above.\n",
    "\n",
    "We're also going to make use of Geoviews which adds some extra functionality in the Bokeh/Datashader/Holoviews family for working with geographical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datashader as ds\n",
    "# import datashader.transfer_functions as tf\n",
    "# from datashader.colors import colormap_select, Greys9\n",
    "from datashader.utils import lnglat_to_meters as webm\n",
    "\n",
    "from holoviews.element import tiles\n",
    "from holoviews.operation.datashader import datashade, dynspread\n",
    "from holoviews.streams import RangeXY\n",
    "\n",
    "import geoviews as gv\n",
    "import cartopy.crs as crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to convert our lat long into a map projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_org_locs_df['easting'], gtr_org_locs_df['northing'] = webm(\n",
    "    gtr_org_locs_df['longitude'], gtr_org_locs_df['latitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fetch some background map tile images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{Z}/{Y}/{X}.jpg'\n",
    "map_tiles = gv.WMTS(url, crs=crs.GOOGLE_MERCATOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colour - An Aside\n",
    "\n",
    "So far, we've been using palettes and colourmaps that come with the plotting libraries. There are also other libraries that provide premade palettes. \n",
    "\n",
    "**Colorcet** is one of these and it focuses on providing *perceptually linear* palettes. Perceptually linear palettes are incrementally changing and account for the percieved difference in intensity of different colours by the human eye to provide a colour scale that is linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorcet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's datashade these organisation locations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width=600\n",
    "height=600\n",
    "cmap = colorcet.kbc\n",
    "\n",
    "opts = dict(width=width, height=height, x_sampling=1, y_sampling=1, cmap=cmap, dynamic=False)\n",
    "tile_opts  = dict(width=width, height=height, xaxis=None, yaxis=None, bgcolor='white', show_grid=False)\n",
    "\n",
    "def make_view(x_range, y_range, **kwargs):\n",
    "    tiles = map_tiles.options(alpha=0.5, **tile_opts)\n",
    "    points = hv.Points(gtr_org_locs_df, ['easting', 'northing'])\n",
    "    d = dynspread(datashade(points, x_range=x_range, y_range=y_range, **opts), shape='circle', threshold=.1)\n",
    "    return d * tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmap = hv.DynamicMap(make_view, streams=[RangeXY()])\n",
    "plot = hv.renderer('bokeh').instance(mode='server').get_plot(dmap)\n",
    "dmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a set of documents that relate to the United Nations Sustainable Development Goals (SDGs). We have seen how to turn text into vectors, but how can we visualise large dimensional text?\n",
    "\n",
    "Here we are going to use Bokeh, as well as TSNE, a useful dimensionality reduction algorithm for visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from im_tutorials.data.sdg import sdg_web_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sdg = sdg_web_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sdg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_definitions = {\n",
    "     1: '1. No Poverty',\n",
    "     2: '2. Zero Hunger',\n",
    "     3: '3. Good Health & Well-being',\n",
    "     4: '4. Quality Education',\n",
    "     5: '5. Gender Equality',\n",
    "     6: '6. Clean Water & Sanitation',\n",
    "     7: '7. Affordable & Clean Energy',\n",
    "     8: '8. Decent Work & Economic Growth',\n",
    "     9: '9. Industry, Innovation & Infrastructure',\n",
    "     10: '10.  Reduced Inequalities',\n",
    "     11: '11.  Sustainable Cities & Communities',\n",
    "     12: '12.  Responsible Consumption & Production',\n",
    "     13: '13.  Climate Action',\n",
    "     14: '14.  Life Below Water',\n",
    "     15: '15.  Life on Land',\n",
    "     16: '16.  Peace, Justice & Strong Institutions',\n",
    "     17: '17.  Partnerships for the Goals'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sdg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_goal(l, goal=17):\n",
    "    new_goals = [g for g in l if g != goal]\n",
    "    return new_goals\n",
    "\n",
    "df_sdg['sdg_goals'] = df_sdg['sdg_goals'].apply(remove_goal)\n",
    "\n",
    "df_sdg['n_goals'] = [len(x) for x in df_sdg['sdg_goals']]\n",
    "df_sdg = df_sdg[(df_sdg['n_goals'] > 0) & (df_sdg['n_goals'] < 4)]\n",
    "\n",
    "df_sdg = df_sdg[df_sdg['text'].str.len() > 140]\n",
    "df_sdg = df_sdg.drop_duplicates('text')\n",
    "df_sdg = df_sdg.drop('index', axis=1)\n",
    "df_sdg = df_sdg.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sdg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from im_tutorials.data.sdg import sdg_web_articles\n",
    "from im_tutorials.features.text_preprocessing import *\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [list(chain(*tokenize_document(document))) for document in df_sdg['text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "term_counts = Counter(chain(*tokenized))\n",
    "term_counts.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['development', 'sdg', 'new', 'global', 'also', 'including', 'support', 'international',\n",
    "             'report', 'implementation', 'national', 'said', 'agenda', 'meeting', 'regional']\n",
    "text_clean = [' '.join([t for t in d if t not in stop_words]) for d in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE, Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tfidf\n",
    "tfidf = TfidfVectorizer(text_clean, min_df=10, max_df=0.5)\n",
    "tfidf_vecs = tfidf.fit_transform(text_clean)\n",
    "\n",
    "# reduce dimensions to 30\n",
    "svd = TruncatedSVD(n_components=30)\n",
    "svd_vecs = svd.fit_transform(tfidf_vecs)\n",
    "\n",
    "# use tsne to create x, y positions\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_vecs = tsne.fit_transform(svd_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.palettes import Category20_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sdg.reset_index(inplace=True)\n",
    "single_goals = (df_sdg[df_sdg['n_goals'] == 1]).index.values\n",
    "tsne_vecs_single = tsne_vecs[single_goals]\n",
    "goal_labels_single = [g[0] for g in df_sdg['sdg_goals'][single_goals]]\n",
    "titles_single = df_sdg['title'][single_goals].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [Category20_16[g-1] for g in goal_labels_single]\n",
    "\n",
    "cds = ColumnDataSource(data={\n",
    "    'tsne_0': tsne_vecs[:, 0][single_goals],\n",
    "    'tsne_1': tsne_vecs[:, 1][single_goals],\n",
    "    'color': colors,\n",
    "    'goal': [sdg_definitions[g] for g in goal_labels_single],\n",
    "    'title': titles_single,\n",
    "    'id': single_goals\n",
    "})\n",
    "\n",
    "p = figure(width=600, height=500, \n",
    "           title='TSNE Plot of Single SDG Article Vectors')\n",
    "\n",
    "hover = HoverTool(\n",
    "    tooltips=[\n",
    "        ('Goal', '@goal'), \n",
    "        ('Title', '@title'), \n",
    "        ('Doc ID', '@id')]\n",
    ")\n",
    "\n",
    "p.circle(source=cds, x='tsne_0', y='tsne_1', color='color', line_width=0, legend='goal', radius=0.4, alpha=0.9)\n",
    "p.add_tools(hover)\n",
    "\n",
    "p.legend.label_text_font_size = '6pt'\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What does this tell you about the overlap of language between the articles that discuss the different SDGs? \n",
    "- Can you use the hover interactivity to investigate the human labelling accuracy in mixed clusters?\n",
    "\n",
    "**Task**\n",
    "- Add legend muting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from im_tutorials.data.gtr import gtr_sample\n",
    "gtr_sample_df = gtr_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cooccurrence Networks\n",
    "\n",
    "We are going to define communities of research topics as groups of topics which commonly occur together. An effective way of finding these clusters, and visualising the results, is by creating a topic cooccurrence network.\n",
    "\n",
    "A cooccurrence graph is a network structure, where nodes are elements and an edge represents the elements of two nodes having cooccured at least once. The edges can then be \"weighted\" by the frequencies of each cooccurring pair. In the case of our research projects, we can say that two topics have cooccurred if they appear in at least one project together. To find all cooccurrences we therefore need to find the pairwise combinations of research topics for every project. For example, a single project with the topics\n",
    "```\n",
    "['Materials Characterisation', 'High Performance Computing', 'Condensed Matter Physics']\n",
    "```\n",
    "\n",
    "will become a set of topic pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combinations function from itertools generates all the possible\n",
    "# elements of combinations from a list with length  r.\n",
    "topics = ['Materials Characterisation', 'High Performance Computing', 'Condensed Matter Physics']\n",
    "list(itertools.combinations(topics, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a cooccurrence network across all projects in our dataset, we will use a Python list comprehension, and then chain togeher all of the cooccurring pairs into one long list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate every pair combination of research topics from each project.\n",
    "# Each pair is sorted alphabetically to make sure that there is only one \n",
    "# possible permutation of each edge.\n",
    "cooccurrences = []\n",
    "\n",
    "for topics in gtr_sample_df['research_topics']:\n",
    "    topic_pairs = itertools.combinations(topics, 2)\n",
    "    for pair in topic_pairs:\n",
    "        cooccurrences.append(tuple(sorted(pair)))\n",
    "\n",
    "# The same can be achieved in this one-liner\n",
    "# cooccurrences = list(\n",
    "# chain(*[[tuple(sorted(c)) for c in (itertools.combinations(d, 2))] for d in gtr_sample_df['research_topics']])\n",
    "# )\n",
    "\n",
    "# Count the frequency of each cooccurring pair.\n",
    "research_topic_co_counter = Counter(cooccurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top Research Topic Cooccurrences by Frequency\", '\\n')\n",
    "print('{:<70}{}'.format('Cooccurrence', 'Frequency'))\n",
    "for k, v in research_topic_co_counter.most_common(20):\n",
    "    topics = k[0] + ' <---> ' + k[1]\n",
    "    print(f'{topics:<70}{v}')\n",
    "    \n",
    "print('\\nMedian Topic Cooccurrence Freqency:')\n",
    "print(np.median(list(research_topic_co_counter.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising Edge Weights\n",
    "\n",
    "Looking at the most frequently cooccurring topics we can pairs that make intuitive sense and are all generally captured neatly within higher order academic disciplines.\n",
    "\n",
    "However this, along with the individual topic frequencies, also shows us that using the cooccurrence frequency as our edge weight might not be such a good idea. High frequency elements are simply more likely to cooccur due to chance. Therefore we should normalise our edge weights. One method for this is to calculate the association strength is a an edge weight where the cooccurrence freqency is normalised by the product of the individual terms' occurrence counts. It is defined as\n",
    "\n",
    "$$ a = \\frac{2 n c_{ij}}{o_{i}o_{j}} $$\n",
    "\n",
    "where $n$ is the total number of elements, $c_{ij}$ is the number of cooccurrences between elements $i$ and $j$, and $o_{i}$ and $o_{j}$ are the individual frequency counts of each element.\n",
    "\n",
    "To build our cooccurrence network, we need to generate a list of unique edges from our long list of cooccurrences and then calculate the association strength for each edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_strength(combo, occurrences, cooccurrences, total):\n",
    "    '''association_strength\n",
    "    Calculates the association strength between a cooccurring pair.\n",
    "    '''\n",
    "    a_s = ((2 * total * cooccurrences[combo]) / \n",
    "           (occurrences[combo[0]] * occurrences[combo[1]]))\n",
    "    return a_s\n",
    "\n",
    "research_topic_counter = Counter(chain(*gtr_sample_df['research_topics']))\n",
    "\n",
    "# Generate a set of cooccurences (a list of unique pairs).\n",
    "# This will form the edges of our cooccurrence graph.\n",
    "edges = set(cooccurrences)\n",
    "# Calculate the total number of elements\n",
    "n = len(list(itertools.chain(*gtr_sample_df['research_topics'])))\n",
    "# Calculate the association strength for each edge.\n",
    "# We take the log of the association strength to give it\n",
    "# a normal distribution.\n",
    "assoc_strengths = np.log10([association_strength(\n",
    "    edge,\n",
    "    research_topic_counter, \n",
    "    research_topic_co_counter, \n",
    "    n) for edge in edges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(assoc_strengths, bins=50)\n",
    "ax.set_xlabel('Log10 Association Strength');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the association strengths shows a fairly smooth normal distribution. We can see that without applying a logarithm, there would be weights in our graph 100,000 times larger than others!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has 3 main tools for working with networks: [`networkx`](https://networkx.github.io/), [`igraph`](https://igraph.org/redirect.html) and [`graph-tool`](https://graph-tool.skewed.de). The first of these, `networkx`, is easy to install and interacting with it is straightforward. It is suitable for networks with up to hundreds of thousands of nodes or edges. With very large networks, it is recommended to use `graph-tool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "weighted_edges = []\n",
    "for (s, t), a_s in zip(edges, assoc_strengths):\n",
    "    weighted_edges.append((s, t, a_s))\n",
    "\n",
    "g = nx.Graph()\n",
    "g.add_weighted_edges_from(weighted_edges, weight='association_strength')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `python-louvain` imports as `community`\n",
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = community.best_partition(g, resolution=0.6, random_state=42, weight='association_strength')\n",
    "n_communities = len(set(part.values()))\n",
    "print('{} communities detected.'.format(n_communities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.palettes import Category20, Spectral4\n",
    "from bokeh.models import MultiLine, TapTool\n",
    "from bokeh.models.graphs import from_networkx, NodesAndLinkedEdges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we make the plot, we will add some extra properties to the nodes in our network. First, we will give each node an attribute, `topic_name`, which is the name of the research topic that the node represents. Second, we will give the node a colour based on the community to which it belongs.\n",
    "\n",
    "Note: This code will break if more than 20 communities are used. In this situation a different colour palette would be needed, or a different way of selecting colours from a small palette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {k: k for k, _ in part.items()}\n",
    "nx.set_node_attributes(g, names, name='topic_name')\n",
    "community_colors = {k: Category20[n_communities][c] for k, c in part.items()}\n",
    "nx.set_node_attributes(g, community_colors, name='color')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print a node to see the properties it holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.nodes['Materials Characterisation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot our network on a 2 dimensional plane, we will need to calculate coordinates for each node. There are read-made algorithms for positioning network nodes visually, and some are built in to `networkx`. The spring layout tries to position nodes according to their edges and relative levels of attraction based on edge weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(g, weight='association_strength', scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need to make a nice plot. Luckily, `bokeh` has built-in support for `networkx` graphs, which makes plotting and interacting with them easy.\n",
    "\n",
    "You can read more about this and see examples in the [Bokeh network visualisation documentation](https://bokeh.pydata.org/en/latest/docs/user_guide/graph.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot and give it some basic features.\n",
    "plot = figure(title=\"Research Topic Cooccurrence Network\",\n",
    "              x_range=(-2.1,2.1), y_range=(-2.1,2.1),\n",
    "             )\n",
    "\n",
    "# Use the renderer built in to `bokeh` to transform our Graph\n",
    "# object into something that `bokeh` can plot.\n",
    "graph_renderer = from_networkx(g, pos, center=(0,0))\n",
    "# Draw glyphs for our nodes and assign properties for interactions.\n",
    "# These include how the nodes interact to clicks and hovering.\n",
    "graph_renderer.node_renderer.glyph = Circle(size=7, fill_color='color', line_color=None)\n",
    "graph_renderer.node_renderer.selection_glyph = Circle(size=7, fill_color='color')\n",
    "graph_renderer.node_renderer.hover_glyph = Circle(size=7, fill_color='color')\n",
    "graph_renderer.node_renderer.muted_glyph = Circle(size=7, fill_color='color', fill_alpha=0.9)\n",
    "# Draw glyphs for edges and assign properties for interactions.\n",
    "graph_renderer.edge_renderer.glyph = MultiLine(line_color=\"#CCCCCC\", line_alpha=0.2, line_width=1)\n",
    "graph_renderer.edge_renderer.selection_glyph = MultiLine(line_color=Spectral4[2], line_width=1.5)\n",
    "graph_renderer.edge_renderer.hover_glyph = MultiLine(line_color=Spectral4[1], line_width=1.5)\n",
    "# Add the ability to select nodes.\n",
    "graph_renderer.selection_policy = NodesAndLinkedEdges()\n",
    "# Add a hover tool, that allows us to investigate nodes with a tooltip. \n",
    "node_hover_tool = HoverTool(tooltips=[(\"Topic\", \"@topic_name\")])\n",
    "# Put everything on the plot.\n",
    "plot.add_tools(node_hover_tool, TapTool())\n",
    "plot.renderers.append(graph_renderer)\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Datashader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have a nice node layout, and visible communities, we have lots of overlapping edges and understanding the graph edge structure is pretty hard. Luckily we can use Datashader to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datashader.layout import random_layout, circular_layout, forceatlas2_layout\n",
    "from datashader.bundling import connect_edges, hammer_bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nx_layout(graph, layout):\n",
    "    data = [[node]+layout[node].tolist() for node in graph.nodes]\n",
    "\n",
    "    nodes = pd.DataFrame(data, columns=['id', 'x', 'y'])\n",
    "    nodes.set_index('id', inplace=True)\n",
    "\n",
    "    edges = pd.DataFrame(list(graph.edges), columns=['source', 'target'])\n",
    "    return nodes, edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, edges = nx_layout(g, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw = 0.1\n",
    "r_nodes = hv.Points(nodes)\n",
    "r_bundled = hv.Curve(hammer_bundle(r_nodes.data, r_edges.data, initial_bandwidth=bw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datashade.cmap=colorcet.fire[40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_network = dynspread(datashade(r_nodes, cmap=[\"cyan\"])) * datashade(r_bundled, **ds_opts)\n",
    "ds_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples of using Datashader for networks can be found [here](https://datashader.org/user_guide/Networks.html), including a great example of visualising UK [research collaboration networks](https://anaconda.org/jbednar/uk_researchers/notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "- Make this look pretty!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try It Yourself\n",
    "\n",
    "1. Import a dataset (or use one from above)\n",
    "2. Pick a one or two variables you want to visualise (or something more complex like a network)\n",
    "3. Plot with Bokeh, Holoviews, and/or Datashader\n",
    "4. Change the options to add interactivity and customise the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "- [Bokeh](https://docs.bokeh.org/en/latest/index.html)\n",
    "- [Holoviews](http://holoviews.org/)\n",
    "- [Datashader](http://datashader.org)\n",
    "- [Geoviews](http://geoviews.org/)\n",
    "\n",
    "\n",
    "### Other Interactive Python Visualisation Packages\n",
    "- [Altair](https://altair-viz.github.io)\n",
    "- [Plotly](https://plot.ly/python/)\n",
    "\n",
    "\n",
    "### For Dashboards\n",
    "- Holoviews\n",
    "- Bokeh Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
